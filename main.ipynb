{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908e8499",
   "metadata": {},
   "source": [
    "## Francisco Teixeira Rocha Aragão - 2021031726\n",
    "## Lorenzo Carneiro Magalhães - 2021031505\n",
    "\n",
    "### Implementação do problema de Entity Search - Information Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d754d",
   "metadata": {},
   "source": [
    "### Primeira submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf296d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "# definindo constantes para normalização dos textos\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "PUNCT = set(string.punctuation)\n",
    "REMOVE_SPACE = re.compile(r\"\\s+\") # regex para substituir múltiplos espaços por um único espaço\n",
    "\n",
    "def normalize(text: str) -> List[str]:\n",
    "     \"\"\"\n",
    "     • lower-case\n",
    "     • tokeniza com wordpunct_tokenize\n",
    "     • remove stop-words / pontuação\n",
    "     • descarta tokens de 1 caractere\n",
    "     \"\"\"\n",
    "     text = REMOVE_SPACE.sub(\" \", text.lower())\n",
    "     tokens = [\n",
    "         t for t in wordpunct_tokenize(text)\n",
    "         if t not in STOPWORDS and t not in PUNCT and len(t) > 1\n",
    "     ]\n",
    "     return tokens\n",
    "\n",
    "\n",
    "# definindo constantes e caminhos para arquivos\n",
    "DATA_DIR     = Path(\"data\")\n",
    "CORPUS_PATH  = DATA_DIR / \"corpus.jsonl\"\n",
    "TEST_PATH    = DATA_DIR / \"test_queries.csv\"\n",
    "SUBM_PATH    = Path(\"submission.csv\")\n",
    "TOP_K        = 100  # máx. de entidades por query como descrito no enunciado\n",
    "\n",
    "\n",
    "# salvando estruturas de indice pra armazenar informações do corpus\n",
    "docs_tokens: List[List[str]] = []\n",
    "entity_ids: List[str]        = []\n",
    "\n",
    "with CORPUS_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"corpus.jsonl\"):\n",
    "        doc = json.loads(line)\n",
    "\n",
    "        # concatena campos relevantes de cada documento no corpus\n",
    "        combined = \" \".join([\n",
    "            doc.get(\"title\", \"\"),\n",
    "            doc.get(\"text\",  \"\"),\n",
    "            \" \".join(doc.get(\"keywords\", [])),\n",
    "        ])\n",
    "\n",
    "        docs_tokens.append(normalize(combined))\n",
    "        entity_ids.append(doc[\"id\"])\n",
    "\n",
    "bm25 = BM25Okapi(docs_tokens)\n",
    "print(f\"Num : {len(entity_ids):,} documentos indexados.\\n\")\n",
    "\n",
    "\n",
    "print(\"ranking BM25\")\n",
    "rows_out: List[List[str]] = []\n",
    "\n",
    "# pegando os scores de cada query e salvando os resultados\n",
    "with TEST_PATH.open(encoding=\"utf-8\") as f:\n",
    "    \n",
    "    reader = csv.DictReader(f)\n",
    "    \n",
    "    for row in tqdm(reader, desc=\"test_queries.csv\"):\n",
    "        \n",
    "        qid, query = row[\"QueryId\"], row[\"Query\"]\n",
    "        \n",
    "        q_tokens   = normalize(query)\n",
    "\n",
    "        if not q_tokens: # isso é só pra evitar queries vazias, idealmente não ocorre \n",
    "            continue\n",
    "        \n",
    "        scores   = bm25.get_scores(q_tokens)\n",
    "        best_idx = sorted(range(len(scores)),\n",
    "                          key=scores.__getitem__, reverse=True)[:TOP_K]\n",
    "        \n",
    "        rows_out.extend([[qid, entity_ids[i]] for i in best_idx])\n",
    "\n",
    "print(f\"total  de linhas na saída: {len(rows_out):,}\\n\")\n",
    "\n",
    "\n",
    "# escrevendo arquivo de saida com os resultados\n",
    "with SUBM_PATH.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    writer.writerow([\"QueryId\", \"EntityId\"])\n",
    "    writer.writerows(rows_out)\n",
    "\n",
    "print(f\"fim\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980087d0",
   "metadata": {},
   "source": [
    "### Segunda submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aad9c7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert: 4641784it [00:37, 123545.53it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv, json, tqdm, numpy as np\n",
    "from pathlib import Path\n",
    "#from pyserini.search import SimpleSearcher\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# caminhos aos arquivos importantes\n",
    "DATA      = Path(\"data/ir-20251-rc\")\n",
    "TEST_FILE = DATA / \"test_queries.csv\"\n",
    "SUBM_FILE = Path(\"submission.csv\")\n",
    "INDEX_DIR = Path(\"index_entities\")\n",
    "\n",
    "# parametros do pipeline de ranking\n",
    "CAND_K = 1000   # numero de candidatos que recupero inicialmente\n",
    "FINAL_K= 100    # numero final de entidades\n",
    "W_CE = 0.7    # peso do cross-encoder na interpolação com o bm25\n",
    "\n",
    "# trabalhando com formatação do corpus para Anserini\n",
    "path_in  = DATA/ \"corpus.jsonl\"\n",
    "\n",
    "os.makedirs(\"corp_anserini\", exist_ok=True)\n",
    "path_out = Path(\"corp_anserini\", \"corpus.jsonl\")\n",
    "\n",
    "\n",
    "with path_in.open(encoding=\"utf-8\") as fin, \\\n",
    "     path_out.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in tqdm.tqdm(fin, desc=\"convert\"):\n",
    "        obj = json.loads(line)\n",
    "        contents = \" \".join([\n",
    "            obj.get(\"title\",\"\"),\n",
    "            \" \".join(obj.get(\"keywords\", [])),\n",
    "            obj.get(\"text\",\"\")\n",
    "        ])\n",
    "        fout.write(json.dumps({\"id\": obj[\"id\"], \"contents\": contents}) + \"\\n\")\n",
    "\n",
    "# essa variavel de ambiente evita que o java use muita memoria\n",
    "# instalei o java com : sudo apt install openjdk-17-sdk\n",
    "os.environ['_JAVA_OPTIONS'] = '-Xms4g -Xmx24g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7281d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: -Xms4g -Xmx24g\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 14:51:10,783 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:205) - Setting log level to INFO\n",
      "2025-06-21 14:51:10,785 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:208) - ============ Loading Index Configuration ============\n",
      "2025-06-21 14:51:10,785 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:209) - AbstractIndexer settings:\n",
      "2025-06-21 14:51:10,785 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:210) -  + DocumentCollection path: ./corp_anserini\n",
      "2025-06-21 14:51:10,785 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:211) -  + CollectionClass: JsonCollection\n",
      "2025-06-21 14:51:10,786 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:212) -  + Index path: index_entities\n",
      "2025-06-21 14:51:10,786 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:213) -  + Threads: 16\n",
      "2025-06-21 14:51:10,786 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:214) -  + Optimize (merge segments)? false\n",
      "2025-06-21 14:51:10,811 INFO  [main] index.IndexCollection (IndexCollection.java:246) - Using DefaultEnglishAnalyzer\n",
      "2025-06-21 14:51:10,811 INFO  [main] index.IndexCollection (IndexCollection.java:247) - Stemmer: porter\n",
      "2025-06-21 14:51:10,811 INFO  [main] index.IndexCollection (IndexCollection.java:248) - Keep stopwords? false\n",
      "2025-06-21 14:51:10,811 INFO  [main] index.IndexCollection (IndexCollection.java:249) - Stopwords file: null\n",
      "2025-06-21 14:51:10,902 INFO  [main] index.IndexCollection (IndexCollection.java:197) - IndexCollection settings:\n",
      "2025-06-21 14:51:10,902 INFO  [main] index.IndexCollection (IndexCollection.java:198) -  + Generator: DefaultLuceneDocumentGenerator\n",
      "2025-06-21 14:51:10,902 INFO  [main] index.IndexCollection (IndexCollection.java:199) -  + Language: en\n",
      "2025-06-21 14:51:10,902 INFO  [main] index.IndexCollection (IndexCollection.java:200) -  + Stemmer: porter\n",
      "2025-06-21 14:51:10,903 INFO  [main] index.IndexCollection (IndexCollection.java:201) -  + Keep stopwords? false\n",
      "2025-06-21 14:51:10,903 INFO  [main] index.IndexCollection (IndexCollection.java:202) -  + Stopwords: null\n",
      "2025-06-21 14:51:10,903 INFO  [main] index.IndexCollection (IndexCollection.java:203) -  + Store positions? true\n",
      "2025-06-21 14:51:10,903 INFO  [main] index.IndexCollection (IndexCollection.java:204) -  + Store docvectors? true\n",
      "2025-06-21 14:51:10,903 INFO  [main] index.IndexCollection (IndexCollection.java:205) -  + Store document \"contents\" field? false\n",
      "2025-06-21 14:51:10,904 INFO  [main] index.IndexCollection (IndexCollection.java:206) -  + Store document \"raw\" field? true\n",
      "2025-06-21 14:51:10,904 INFO  [main] index.IndexCollection (IndexCollection.java:207) -  + Additional fields to index: []\n",
      "2025-06-21 14:51:10,904 INFO  [main] index.IndexCollection (IndexCollection.java:208) -  + Whitelist: null\n",
      "2025-06-21 14:51:10,904 INFO  [main] index.IndexCollection (IndexCollection.java:209) -  + Pretokenized?: false\n",
      "2025-06-21 14:51:10,904 INFO  [main] index.IndexCollection (IndexCollection.java:210) -  + Codec: Lucene99\n",
      "2025-06-21 14:51:10,904 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:238) - ============ Indexing Collection ============\n",
      "2025-06-21 14:51:10,907 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:247) - Thread pool with 16 threads initialized.\n",
      "2025-06-21 14:51:10,908 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:248) - 1 file found in ./corp_anserini\n",
      "2025-06-21 14:51:10,908 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:249) - Starting to index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jun 21, 2025 2:51:10 PM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 14:52:10,912 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:259) - 1,330,000 documents indexed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    'python', '-m', 'pyserini.index.lucene',\n",
    "    '-collection', 'JsonCollection',\n",
    "    '-generator', 'DefaultLuceneDocumentGenerator',\n",
    "    '-input', './corp_anserini',\n",
    "    '-index', 'index_entities',\n",
    "    '-threads', '16',\n",
    "    '-storePositions', '-storeDocvectors', '-storeRaw'\n",
    "]\n",
    "\n",
    "subprocess.run(cmd, check=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f148a442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jun 21, 2025 2:19:48 PM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n",
      "Queries: 13it [09:04, 41.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# agora que tenho os candidatos, uso o cross-encoder para re-ranquear esses candidatos\u001b[39;00m\n\u001b[1;32m     26\u001b[0m texts \u001b[38;5;241m=\u001b[39m [searcher\u001b[38;5;241m.\u001b[39mdoc(did)\u001b[38;5;241m.\u001b[39mraw() \u001b[38;5;28;01mfor\u001b[39;00m did \u001b[38;5;129;01min\u001b[39;00m cand_ids]\n\u001b[0;32m---> 27\u001b[0m ce_scores \u001b[38;5;241m=\u001b[39m \u001b[43mce\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m ce_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(cand_ids, ce_scores))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# jutno os scores com interpolação\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/sentence_transformers/cross_encoder/util.py:68\u001b[0m, in \u001b[0;36mcross_encoder_predict_rank_args_decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mpop(deprecated_arg)\n\u001b[1;32m     64\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     65\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe CrossEncoder.predict `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_arg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` argument is deprecated and has no effect. It will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         )\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:651\u001b[0m, in \u001b[0;36mCrossEncoder.predict\u001b[0;34m(self, sentences, batch_size, show_progress_bar, activation_fn, apply_softmax, convert_to_numpy, convert_to_tensor)\u001b[0m\n\u001b[1;32m    644\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    645\u001b[0m     batch,\n\u001b[1;32m    646\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    647\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[1;32m    650\u001b[0m features\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 651\u001b[0m model_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(model_predictions\u001b[38;5;241m.\u001b[39mlogits)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apply_softmax \u001b[38;5;129;01mand\u001b[39;00m logits\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1503\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1503\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1515\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1517\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1016\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1016\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1029\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:662\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    651\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    652\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    653\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    659\u001b[0m         output_attentions,\n\u001b[1;32m    660\u001b[0m     )\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 662\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    672\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:594\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    591\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    592\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 594\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/transformers/pytorch_utils.py:253\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:606\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 606\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:506\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 506\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ir25/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# agora sim inicio das tarefas de ranqueamento\n",
    "searcher = LuceneSearcher(str(INDEX_DIR))\n",
    "searcher.set_bm25(k1=0.92, b=0.36) # parametros do bm25, k1 controla a sensibilidade ao tamanho do documento, b controla a normalização\n",
    "searcher.set_rm3(fb_terms=10, fb_docs=50, original_query_weight=0.5) # parâmetros do RM3, fb_terms é o número de termos de feedback, fb_docs é o número de documentos de feedback, original_query_weight é o peso da query original\n",
    "\n",
    "ce = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "\n",
    "# como vou fazer interpolação dos modelos, preciso normalizar os scores dos metodos para poder agregalos\n",
    "def normalize_scores(d):\n",
    "    vals = np.array(list(d.values()))\n",
    "    return {k: (v - vals.min()) / (np.ptp(vals) + 1e-9) for k, v in d.items()}\n",
    "\n",
    "rows_out = []\n",
    "\n",
    "with TEST_FILE.open() as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in tqdm.tqdm(reader, desc=\"Queries\"):\n",
    "        qid, query = row[\"QueryId\"], row[\"Query\"]\n",
    "\n",
    "        # inicialmente uso rm3 + bm25 para achar candidatos relevantes para a query\n",
    "        hits = searcher.search(query, CAND_K)\n",
    "        cand_ids  = [h.docid for h in hits]\n",
    "        bm25_dict = {h.docid: h.score for h in hits}\n",
    "\n",
    "        # agora que tenho os candidatos, uso o cross-encoder para re-ranquear esses candidatos\n",
    "        texts = [searcher.doc(did).raw() for did in cand_ids]\n",
    "        ce_scores = ce.predict([(query, t) for t in texts], batch_size=32)\n",
    "        ce_dict = dict(zip(cand_ids, ce_scores))\n",
    "\n",
    "        # jutno os scores com interpolação\n",
    "        b_norm = normalize_scores(bm25_dict)\n",
    "        c_norm = normalize_scores(ce_dict)\n",
    "        final_scores = {d: W_CE*c_norm[d] + (1-W_CE)*b_norm[d] for d in cand_ids}\n",
    "\n",
    "        top_ids = sorted(final_scores, key=final_scores.get, reverse=True)[:FINAL_K]\n",
    "        rows_out.extend([[qid, did] for did in top_ids])\n",
    "\n",
    "# salvo os resultados obtidos \n",
    "with SUBM_FILE.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"QueryId\", \"EntityId\"])\n",
    "    writer.writerows(rows_out)\n",
    "\n",
    "print(\"fim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1f0cf",
   "metadata": {},
   "source": [
    "## Terceira submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512396a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv, json, tqdm, numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "#from pyserini.search import SimpleSearcher\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from sentence_transformers import CrossEncoder\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# definindo constantes para normalização dos textos\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "PUNCT = set(string.punctuation)\n",
    "REMOVE_SPACE = re.compile(r\"\\s+\") # regex para substituir múltiplos espaços por um único espaço\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "     \"\"\"\n",
    "     • lower-case\n",
    "     • tokeniza com wordpunct_tokenize\n",
    "     • remove stop-words / pontuação\n",
    "     • descarta tokens de 1 caractere\n",
    "     \"\"\"\n",
    "     text = REMOVE_SPACE.sub(\" \", text.lower())\n",
    "     tokens = [\n",
    "         t for t in wordpunct_tokenize(text)\n",
    "         if t not in STOPWORDS and t not in PUNCT and len(t) > 1\n",
    "     ]\n",
    "     return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001469b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "/home/francisco/miniconda3/envs/ir25/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "convert: 4641784it [03:24, 22689.42it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# caminhos aos arquivos importantes\n",
    "DATA      = Path(\"data/ir-20251-rc\")\n",
    "TEST_FILE = DATA / \"test_queries.csv\"\n",
    "SUBM_FILE = Path(\"submission.csv\")\n",
    "INDEX_DIR = Path(\"index_entities\")\n",
    "\n",
    "\n",
    "\n",
    "# trabalhando com formatação do corpus para Anserini\n",
    "path_in  = DATA/ \"corpus.jsonl\"\n",
    "\n",
    "os.makedirs(\"corp_anserini\", exist_ok=True)\n",
    "path_out = Path(\"corp_anserini\", \"corpus.jsonl\")\n",
    "\n",
    "\n",
    "with path_in.open(encoding=\"utf-8\") as fin, \\\n",
    "     path_out.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in tqdm.tqdm(fin, desc=\"convert\"):\n",
    "        obj = json.loads(line)\n",
    "        contents = \" \".join([\n",
    "            obj.get(\"title\",\"\"),\n",
    "            \" \".join(obj.get(\"keywords\", [])),\n",
    "            obj.get(\"text\",\"\")\n",
    "        ])\n",
    "        fout.write(json.dumps({\"id\": obj[\"id\"], \"contents\": normalize(contents)}) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f09b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: -Xms4g -Xmx24g\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 15:09:53,396 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:205) - Setting log level to INFO\n",
      "2025-06-21 15:09:53,398 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:208) - ============ Loading Index Configuration ============\n",
      "2025-06-21 15:09:53,398 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:209) - AbstractIndexer settings:\n",
      "2025-06-21 15:09:53,398 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:210) -  + DocumentCollection path: ./corp_anserini\n",
      "2025-06-21 15:09:53,399 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:211) -  + CollectionClass: JsonCollection\n",
      "2025-06-21 15:09:53,399 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:212) -  + Index path: index_entities\n",
      "2025-06-21 15:09:53,399 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:213) -  + Threads: 4\n",
      "2025-06-21 15:09:53,399 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:214) -  + Optimize (merge segments)? false\n",
      "2025-06-21 15:09:53,420 INFO  [main] index.IndexCollection (IndexCollection.java:246) - Using DefaultEnglishAnalyzer\n",
      "2025-06-21 15:09:53,421 INFO  [main] index.IndexCollection (IndexCollection.java:247) - Stemmer: porter\n",
      "2025-06-21 15:09:53,421 INFO  [main] index.IndexCollection (IndexCollection.java:248) - Keep stopwords? false\n",
      "2025-06-21 15:09:53,421 INFO  [main] index.IndexCollection (IndexCollection.java:249) - Stopwords file: null\n",
      "2025-06-21 15:09:53,503 INFO  [main] index.IndexCollection (IndexCollection.java:197) - IndexCollection settings:\n",
      "2025-06-21 15:09:53,504 INFO  [main] index.IndexCollection (IndexCollection.java:198) -  + Generator: DefaultLuceneDocumentGenerator\n",
      "2025-06-21 15:09:53,504 INFO  [main] index.IndexCollection (IndexCollection.java:199) -  + Language: en\n",
      "2025-06-21 15:09:53,504 INFO  [main] index.IndexCollection (IndexCollection.java:200) -  + Stemmer: porter\n",
      "2025-06-21 15:09:53,504 INFO  [main] index.IndexCollection (IndexCollection.java:201) -  + Keep stopwords? false\n",
      "2025-06-21 15:09:53,504 INFO  [main] index.IndexCollection (IndexCollection.java:202) -  + Stopwords: null\n",
      "2025-06-21 15:09:53,505 INFO  [main] index.IndexCollection (IndexCollection.java:203) -  + Store positions? true\n",
      "2025-06-21 15:09:53,505 INFO  [main] index.IndexCollection (IndexCollection.java:204) -  + Store docvectors? true\n",
      "2025-06-21 15:09:53,505 INFO  [main] index.IndexCollection (IndexCollection.java:205) -  + Store document \"contents\" field? false\n",
      "2025-06-21 15:09:53,505 INFO  [main] index.IndexCollection (IndexCollection.java:206) -  + Store document \"raw\" field? true\n",
      "2025-06-21 15:09:53,505 INFO  [main] index.IndexCollection (IndexCollection.java:207) -  + Additional fields to index: []\n",
      "2025-06-21 15:09:53,505 INFO  [main] index.IndexCollection (IndexCollection.java:208) -  + Whitelist: null\n",
      "2025-06-21 15:09:53,505 INFO  [main] index.IndexCollection (IndexCollection.java:209) -  + Pretokenized?: false\n",
      "2025-06-21 15:09:53,505 INFO  [main] index.IndexCollection (IndexCollection.java:210) -  + Codec: Lucene99\n",
      "2025-06-21 15:09:53,506 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:238) - ============ Indexing Collection ============\n",
      "2025-06-21 15:09:53,508 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:247) - Thread pool with 4 threads initialized.\n",
      "2025-06-21 15:09:53,508 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:248) - 1 file found in ./corp_anserini\n",
      "2025-06-21 15:09:53,508 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:249) - Starting to index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jun 21, 2025 3:09:53 PM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 15:10:53,510 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:259) - 1,540,000 documents indexed\n",
      "2025-06-21 15:11:53,511 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:259) - 3,080,000 documents indexed\n",
      "2025-06-21 15:12:53,511 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:259) - 4,610,000 documents indexed\n",
      "2025-06-21 15:13:17,629 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:307) - Indexing Complete! 4,641,784 documents indexed\n",
      "2025-06-21 15:13:17,629 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:308) - ============ Final Counter Values ============\n",
      "2025-06-21 15:13:17,629 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:309) - indexed:        4,641,784\n",
      "2025-06-21 15:13:17,630 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:310) - unindexable:            0\n",
      "2025-06-21 15:13:17,630 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:311) - empty:                  0\n",
      "2025-06-21 15:13:17,630 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:312) - skipped:                0\n",
      "2025-06-21 15:13:17,630 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:313) - errors:                 0\n",
      "2025-06-21 15:13:17,636 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:316) - Total 4,641,784 documents indexed in 00:03:24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '-m', 'pyserini.index.lucene', '-collection', 'JsonCollection', '-generator', 'DefaultLuceneDocumentGenerator', '-input', './corp_anserini', '-index', 'index_entities', '-threads', '4', '-storePositions', '-storeDocvectors', '-storeRaw'], returncode=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# essa variavel de ambiente evita que o java use muita memoria\n",
    "# instalei o java com : sudo apt install openjdk-17-sdk\n",
    "os.environ['_JAVA_OPTIONS'] = '-Xms4g -Xmx24g'\n",
    "\n",
    "cmd = [\n",
    "    'python', '-m', 'pyserini.index.lucene',\n",
    "    '-collection', 'JsonCollection',\n",
    "    '-generator', 'DefaultLuceneDocumentGenerator',\n",
    "    '-input', './corp_anserini',\n",
    "    '-index', 'index_entities',\n",
    "    '-threads', '4',\n",
    "    '-storePositions', '-storeDocvectors', '-storeRaw'\n",
    "]\n",
    "\n",
    "subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0810c62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Queries: 233it [36:22,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# parametros do pipeline de ranking\n",
    "CAND_K = 500   # numero de candidatos que recupero inicialmente\n",
    "FINAL_K= 100    # numero final de entidades\n",
    "W_CE = 0.8    # peso do cross-encoder na interpolação com o bm25\n",
    "\n",
    "# agora sim inicio das tarefas de ranqueamento\n",
    "searcher = LuceneSearcher(str(INDEX_DIR))\n",
    "searcher.set_bm25(k1=1.2, b=0.75) # parametros do bm25, k1 controla a sensibilidade ao tamanho do documento, b controla a normalização\n",
    "searcher.set_rm3(fb_terms=10, fb_docs=50, original_query_weight=0.6) # parâmetros do RM3, fb_terms é o número de termos de feedback, fb_docs é o número de documentos de feedback, original_query_weight é o peso da query original\n",
    "\n",
    "ce = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "\n",
    "# como vou fazer interpolação dos modelos, preciso normalizar os scores dos metodos para poder agregalos\n",
    "def normalize_scores(d):\n",
    "    vals = np.array(list(d.values()))\n",
    "    return {k: (v - vals.min()) / (np.ptp(vals) + 1e-9) for k, v in d.items()}\n",
    "\n",
    "rows_out = []\n",
    "\n",
    "with TEST_FILE.open() as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in tqdm.tqdm(reader, desc=\"Queries\"):\n",
    "        qid, query = row[\"QueryId\"], row[\"Query\"]\n",
    "\n",
    "        # inicialmente uso rm3 + bm25 para achar candidatos relevantes para a query\n",
    "        hits = searcher.search(query, CAND_K)\n",
    "        cand_ids  = [h.docid for h in hits]\n",
    "        bm25_dict = {h.docid: h.score for h in hits}\n",
    "\n",
    "        # agora que tenho os candidatos, uso o cross-encoder para re-ranquear esses candidatos\n",
    "        texts = [searcher.doc(did).raw() for did in cand_ids]\n",
    "        ce_scores = ce.predict([(query, t) for t in texts], batch_size=32)\n",
    "        ce_dict = dict(zip(cand_ids, ce_scores))\n",
    "\n",
    "        # jutno os scores com interpolação\n",
    "        b_norm = normalize_scores(bm25_dict)\n",
    "        c_norm = normalize_scores(ce_dict)\n",
    "        final_scores = {d: W_CE*c_norm[d] + (1-W_CE)*b_norm[d] for d in cand_ids}\n",
    "\n",
    "        top_ids = sorted(final_scores, key=final_scores.get, reverse=True)[:FINAL_K]\n",
    "        rows_out.extend([[qid, did] for did in top_ids])\n",
    "\n",
    "# salvo os resultados obtidos \n",
    "with SUBM_FILE.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"QueryId\", \"EntityId\"])\n",
    "    writer.writerows(rows_out)\n",
    "\n",
    "print(\"fim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1704a3",
   "metadata": {},
   "source": [
    "### Quarta submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb1bbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jun 21, 2025 4:06:52 PM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n",
      "Queries: 233it [1:20:11, 20.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# parametros do pipeline de ranking\n",
    "CAND_K = 1000   # numero de candidatos que recupero inicialmente\n",
    "FINAL_K= 100    # numero final de entidades\n",
    "\n",
    "# agora sim inicio das tarefas de ranqueamento\n",
    "searcher = LuceneSearcher(str(INDEX_DIR))\n",
    "searcher.set_bm25(k1=1.2, b=0.75) # parametros do bm25, k1 controla a sensibilidade ao tamanho do documento, b controla a normalização\n",
    "searcher.set_rm3(fb_terms=12, fb_docs=50, original_query_weight=0.6) # parâmetros do RM3, fb_terms é o número de termos de feedback, fb_docs é o número de documentos de feedback, original_query_weight é o peso da query original\n",
    "\n",
    "ce = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "\n",
    "# como vou fazer interpolação dos modelos, preciso normalizar os scores dos metodos para poder agregalos\n",
    "def normalize_scores(d):\n",
    "    vals = np.array(list(d.values()))\n",
    "    return {k: (v - vals.min()) / (np.ptp(vals) + 1e-9) for k, v in d.items()}\n",
    "\n",
    "rows_out = []\n",
    "\n",
    "with TEST_FILE.open() as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in tqdm.tqdm(reader, desc=\"Queries\"):\n",
    "        qid, query = row[\"QueryId\"], row[\"Query\"]\n",
    "\n",
    "        # inicialmente uso rm3 + bm25 para achar candidatos relevantes para a query\n",
    "        hits = searcher.search(query, CAND_K)\n",
    "        cand_ids  = [h.docid for h in hits]\n",
    "        bm25_dict = {h.docid: h.score for h in hits}\n",
    "\n",
    "        # agora que tenho os candidatos, uso o cross-encoder para re-ranquear esses candidatos\n",
    "        texts = [searcher.doc(did).raw() for did in cand_ids]\n",
    "        ce_scores = ce.predict([(query, t) for t in texts], batch_size=32)\n",
    "        ce_dict = dict(zip(cand_ids, ce_scores))\n",
    "\n",
    "        # jutno os scores com interpolação\n",
    "        b_norm = normalize_scores(bm25_dict)\n",
    "        c_norm = normalize_scores(ce_dict)\n",
    "\n",
    "        final_scores = {d: c_norm[d] for d in cand_ids}\n",
    "\n",
    "        top_ids = sorted(final_scores, key=final_scores.get, reverse=True)[:FINAL_K]\n",
    "        rows_out.extend([[qid, did] for did in top_ids])\n",
    "\n",
    "# salvo os resultados obtidos \n",
    "with SUBM_FILE.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"QueryId\", \"EntityId\"])\n",
    "    writer.writerows(rows_out)\n",
    "\n",
    "print(\"fim\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
