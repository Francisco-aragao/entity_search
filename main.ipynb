{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908e8499",
   "metadata": {},
   "source": [
    "## Francisco Teixeira Rocha Aragão - 2021031726\n",
    "## Lorenzo Carneiro Magalhães - 2021031505\n",
    "\n",
    "### Implementação do problema de Entity Search - Information Retrieval\n",
    "\n",
    "Abaixo estão as implementações para cada submissão realizada. Vale destacar que os caminhos podem variar, então é importante OU colocar os dados na entrada correta OU alterar os caminhos no código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd6db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import CrossEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo constantes e caminhos para arquivos\n",
    "DATA_DIR     = Path(\"data/ir-20251-rc\")\n",
    "CORPUS_PATH  = DATA_DIR / \"corpus.jsonl\"\n",
    "TEST_PATH    = DATA_DIR / \"test_queries.csv\"\n",
    "SUBM_PATH    = Path(\"submission1.csv\")\n",
    "TOP_K        = 100  # máx de entidades por query como descrito no enunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d754d",
   "metadata": {},
   "source": [
    "## Primeira submissão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f9b7f",
   "metadata": {},
   "source": [
    "Essa primeira submissão é bem simples e implementa o método bm25 sobre o corpus de documentos.\n",
    "\n",
    "O indice é gerenciado manualmente, sendo feito o preprocessamento dos documentos, com as informações sobre IDs e tokens sendo salvas internamente para serem utilizadas no ranking.\n",
    "\n",
    "Eessa etapa é muito custosa pois o índice é salvo em memória, porém conseguimos rodar localmente. Em outras máquinas isso possivelmente pode ser ajustado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf296d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo constantes para normalização dos textos\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "PUNCT = set(string.punctuation)\n",
    "REMOVE_SPACE = re.compile(r\"\\s+\") # regex para substituir múltiplos espaços por um único espaço\n",
    "\n",
    "def normalize(text: str) -> list[str]:\n",
    "     \"\"\"\n",
    "     • lower-case\n",
    "     • tokeniza com wordpunct_tokenize\n",
    "     • remove stop-words / pontuação\n",
    "     • descarta tokens de 1 caractere\n",
    "     \"\"\"\n",
    "     text = REMOVE_SPACE.sub(\" \", text.lower())\n",
    "     tokens = [\n",
    "         t for t in wordpunct_tokenize(text)\n",
    "         if t not in STOPWORDS and t not in PUNCT and len(t) > 1\n",
    "     ]\n",
    "     return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef63bf2",
   "metadata": {},
   "source": [
    "Aqui o corpus é percorrido para gerar o indice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando estruturas de indice pra armazenar informações do corpus\n",
    "docs_tokens: list[list[str]] = []\n",
    "entity_ids: list[str]        = []\n",
    "\n",
    "with CORPUS_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"corpus.jsonl\"):\n",
    "        doc = json.loads(line)\n",
    "\n",
    "        # concatena campos relevantes de cada documento no corpus\n",
    "        combined = \" \".join([\n",
    "            doc.get(\"title\", \"\"),\n",
    "            doc.get(\"text\",  \"\"),\n",
    "            \" \".join(doc.get(\"keywords\", [])),\n",
    "        ])\n",
    "\n",
    "        docs_tokens.append(normalize(combined))\n",
    "        entity_ids.append(doc[\"id\"])\n",
    "\n",
    "bm25 = BM25Okapi(docs_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0edf6",
   "metadata": {},
   "source": [
    "Agora passaremos no bm25 realizando as queries\n",
    "\n",
    "Assim como os docs, as queries também são normalizadas e tokenizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e75ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_out: list[list[str]] = []\n",
    "\n",
    "# pegando os scores de cada query e salvando os resultados\n",
    "with TEST_PATH.open(encoding=\"utf-8\") as f:\n",
    "    \n",
    "    reader = csv.DictReader(f)\n",
    "    \n",
    "    for row in tqdm(reader, desc=\"test_queries.csv\"):\n",
    "        \n",
    "        qid, query = row[\"QueryId\"], row[\"Query\"]\n",
    "        \n",
    "        q_tokens = normalize(query)\n",
    "\n",
    "        scores = bm25.get_scores(q_tokens)\n",
    "        \n",
    "        # pego os melhores candidatos\n",
    "        best_idx = sorted(range(len(scores)),\n",
    "                          key=scores.__getitem__, reverse=True)[:TOP_K]\n",
    "        \n",
    "        rows_out.extend([[qid, entity_ids[i]] for i in best_idx])\n",
    "\n",
    "print(f\"total  de linhas na saída: {len(rows_out):,}\\n\")\n",
    "\n",
    "\n",
    "# escrevendo arquivo de saida com os resultados\n",
    "with SUBM_PATH.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    writer.writerow([\"QueryId\", \"EntityId\"])\n",
    "    writer.writerows(rows_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7726c448",
   "metadata": {},
   "source": [
    "## Indexação do pyserini\n",
    "\n",
    "A partir daqui começamos a utilizar o pyserini para indexar os documentos e realizar as queries.\n",
    "\n",
    "O pyserini é uma biblioteca que facilita a indexação e busca de documentos, no entanto ele tem uma formatação própria para os documentos e queries.\n",
    "\n",
    "Aqui estamos convertando os documentos para o formato esperado pelo pyserini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_DIR = Path(\"index_entities\")\n",
    "\n",
    "# trabalhando com formatação do corpus para Anserini\n",
    "PATH_CORPUS  = DATA_DIR / \"corpus.jsonl\"\n",
    "\n",
    "os.makedirs(\"corp_anserini\", exist_ok=True)\n",
    "path_out = Path(\"corp_anserini\", \"corpus.jsonl\")\n",
    "\n",
    "\n",
    "with PATH_CORPUS.open(encoding=\"utf-8\") as fin, \\\n",
    "     path_out.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in tqdm(fin, desc=\"convert\"):\n",
    "        obj = json.loads(line)\n",
    "        contents = \" \".join([\n",
    "            obj.get(\"title\",\"\"),\n",
    "            \" \".join(obj.get(\"keywords\", [])),\n",
    "            obj.get(\"text\",\"\")\n",
    "        ])\n",
    "        fout.write(json.dumps({\"id\": obj[\"id\"], \"contents\": contents}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e4624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalei o java com : sudo apt install openjdk-17-sdk\n",
    "# configuração do Java\n",
    "os.environ['_JAVA_OPTIONS'] = '-Xms4g -Xmx24g'\n",
    "\n",
    "# só passo os parametros necessarios e os caminhos para os arquivos\n",
    "# o numero de threads pode ser ajustado\n",
    "cmd = [\n",
    "    'python', '-m', 'pyserini.index.lucene',\n",
    "    '-collection', 'JsonCollection',\n",
    "    '-generator', 'DefaultLuceneDocumentGenerator',\n",
    "    '-input', './corp_anserini',\n",
    "    '-index', 'index_entities',\n",
    "    '-threads', '4',\n",
    "    '-storePositions', '-storeDocvectors', '-storeRaw'\n",
    "]\n",
    "\n",
    "subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980087d0",
   "metadata": {},
   "source": [
    "## Segunda submissão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa47fd",
   "metadata": {},
   "source": [
    "Essa submissão é mais robusta do que a primeira, pois além do bm25, são utilizadas outras estratégias como expansão de queries além do  modelo CrossEncoder para realizar o reranking.\n",
    "\n",
    "O gerenciamento do indice é feito pela biblioteca `pyserini`, o que não é controlado manualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a066b5b",
   "metadata": {},
   "source": [
    "Com a biblioteca `pyserini`, o corpus é então indexado e salvo em um diretório especifico. A biblioteca cuida de todo o processo, não sendo feito nada manualmente.\n",
    "Não encontramos biblioteca do python para isso então o comando é feito diretamente via subprocess.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c60f38",
   "metadata": {},
   "source": [
    "Agora rodo o todo o novo pipeline de ranking:\n",
    "\n",
    "Bm25 + rm3 (ranking inicial) -> CrossEncoder (atua sobre ranking inicial de documentos) -> Ranking final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f148a442",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBM_PATH2 = Path(\"submission2.csv\")\n",
    "\n",
    "# parametros do pipeline de ranking\n",
    "CAND_K = 1000   # numero de candidatos que recupero inicialmente\n",
    "FINAL_K = 100    # numero final de entidades\n",
    "W_CE = 0.7    # peso do cross-encoder na interpolação com o bm25\n",
    "\n",
    "# agora sim inicio das tarefas de ranqueamento\n",
    "searcher = LuceneSearcher(str(INDEX_DIR))\n",
    "\n",
    "# parametros do bm25, k1 controla a sensibilidade ao tamanho do documento, b controla a normalização\n",
    "searcher.set_bm25(k1=0.92, b=0.36) \n",
    "\n",
    "# parâmetros do RM3:\n",
    "# fb_terms é o número de termos de feedback,\n",
    "# fb_docs é o número de documentos de feedback,\n",
    "# original_query_weight é o peso da query original\n",
    "searcher.set_rm3(fb_terms=10, fb_docs=50, original_query_weight=0.5) \n",
    "\n",
    "# inicializando o cross-encoder com o modelo pré-treinado\n",
    "# melhor modelo de cross-encoder\n",
    "ce = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "\n",
    "# como vou fazer interpolação dos modelos, preciso normalizar os scores dos metodos para poder agregalos\n",
    "def normalize_scores(d):\n",
    "    vals = np.array(list(d.values()))\n",
    "    return {k: (v - vals.min()) / (np.ptp(vals) + 1e-9) for k, v in d.items()}\n",
    "\n",
    "rows_out = []\n",
    "\n",
    "with TEST_PATH.open() as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in tqdm(reader, desc=\"Queries\"):\n",
    "        qid, query = row[\"QueryId\"], row[\"Query\"]\n",
    "\n",
    "        # inicialmente uso rm3 + bm25 para achar candidatos relevantes para a query\n",
    "        hits = searcher.search(query, CAND_K)\n",
    "        cand_ids  = [h.docid for h in hits]\n",
    "        bm25_dict = {h.docid: h.score for h in hits}\n",
    "\n",
    "        # agora que tenho os candidatos, uso o cross-encoder para re-ranquear esses candidatos\n",
    "        texts = [searcher.doc(did).raw() for did in cand_ids]\n",
    "        ce_scores = ce.predict([(query, t) for t in texts], batch_size=32)\n",
    "        ce_dict = dict(zip(cand_ids, ce_scores))\n",
    "\n",
    "        # jutno os scores com interpolação\n",
    "        b_norm = normalize_scores(bm25_dict)\n",
    "        c_norm = normalize_scores(ce_dict)\n",
    "        final_scores = {d: W_CE*c_norm[d] + (1-W_CE)*b_norm[d] for d in cand_ids}\n",
    "\n",
    "        # pego os melhores candidatos\n",
    "        top_ids = sorted(final_scores, key=final_scores.get, reverse=True)[:FINAL_K]\n",
    "        rows_out.extend([[qid, did] for did in top_ids])\n",
    "\n",
    "# salvo os resultados obtidos \n",
    "with SUBM_PATH2.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"QueryId\", \"EntityId\"])\n",
    "    writer.writerows(rows_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1f0cf",
   "metadata": {},
   "source": [
    "## Terceira submissão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516e432",
   "metadata": {},
   "source": [
    "A ideia agora foi alterar alguns parametros, como o numero de documentos retornados pelo bm25 (que será usado no cross encoder). Também foi alterado o peso da interpolação e os parametros do Bm25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBM_PATH3 = Path(\"submission3.csv\")\n",
    "\n",
    "# parametros do pipeline de ranking\n",
    "CAND_K = 500   # numero de candidatos que recupero inicialmente\n",
    "FINAL_K= 100    # numero final de entidades\n",
    "W_CE = 0.8    # peso do cross-encoder na interpolação com o bm25\n",
    "\n",
    "# agora sim inicio das tarefas de ranqueamento\n",
    "searcher = LuceneSearcher(str(INDEX_DIR))\n",
    "\n",
    "# parametros do bm25, k1 controla a sensibilidade ao tamanho do documento, b controla a normalização\n",
    "searcher.set_bm25(k1=1.2, b=0.75)\n",
    "\n",
    "# parâmetros do RM3:\n",
    "# fb_terms é o número de termos de feedback,\n",
    "# fb_docs é o número de documentos de feedback,\n",
    "# original_query_weight é o peso da query original\n",
    "searcher.set_rm3(fb_terms=10, fb_docs=50, original_query_weight=0.6)\n",
    "\n",
    "# inicializando o cross-encoder com o modelo pré-treinado\n",
    "# melhor modelo de cross-encoder\n",
    "ce = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "\n",
    "# como vou fazer interpolação dos modelos, preciso normalizar os scores dos metodos para poder agregalos\n",
    "def normalize_scores(d):\n",
    "    vals = np.array(list(d.values()))\n",
    "    return {k: (v - vals.min()) / (np.ptp(vals) + 1e-9) for k, v in d.items()}\n",
    "\n",
    "rows_out = []\n",
    "\n",
    "with TEST_PATH.open() as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in tqdm(reader, desc=\"Queries\"):\n",
    "        qid, query = row[\"QueryId\"], row[\"Query\"]\n",
    "\n",
    "        # inicialmente uso rm3 + bm25 para achar candidatos relevantes para a query\n",
    "        hits = searcher.search(query, CAND_K)\n",
    "        cand_ids  = [h.docid for h in hits]\n",
    "        bm25_dict = {h.docid: h.score for h in hits}\n",
    "\n",
    "        # agora que tenho os candidatos, uso o cross-encoder para re-ranquear esses candidatos\n",
    "        texts = [searcher.doc(did).raw() for did in cand_ids]\n",
    "        ce_scores = ce.predict([(query, t) for t in texts], batch_size=32)\n",
    "        ce_dict = dict(zip(cand_ids, ce_scores))\n",
    "\n",
    "        # jutno os scores com interpolação\n",
    "        b_norm = normalize_scores(bm25_dict)\n",
    "        c_norm = normalize_scores(ce_dict)\n",
    "        final_scores = {d: W_CE*c_norm[d] + (1-W_CE)*b_norm[d] for d in cand_ids}\n",
    "\n",
    "        # pego os melhores candidatos\n",
    "        top_ids = sorted(final_scores, key=final_scores.get, reverse=True)[:FINAL_K]\n",
    "        rows_out.extend([[qid, did] for did in top_ids])\n",
    "\n",
    "# salvo os resultados obtidos \n",
    "with SUBM_PATH3.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"QueryId\", \"EntityId\"])\n",
    "    writer.writerows(rows_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1704a3",
   "metadata": {},
   "source": [
    "## Quarta submissão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a6399",
   "metadata": {},
   "source": [
    "Como essa submissão apenas altera os scores finais, não há a necessidade de trabalhar com o corpus novamente igual nas últimas etapas, então essa parte foi ignorada.\n",
    "\n",
    "Assim, agora essa submissão apenas usa o score final totalmente dependente do modelo de cross encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBM_PATH4 = Path(\"submission4.csv\")\n",
    "\n",
    "# parametros do pipeline de ranking\n",
    "CAND_K = 1000   # numero de candidatos que recupero inicialmente\n",
    "FINAL_K= 100    # numero final de entidades\n",
    "\n",
    "# agora sim inicio das tarefas de ranqueamento\n",
    "searcher = LuceneSearcher(str(INDEX_DIR))\n",
    "\n",
    "# parametros do bm25, k1 controla a sensibilidade ao tamanho do documento, b controla a normalização\n",
    "searcher.set_bm25(k1=1.2, b=0.75)\n",
    "\n",
    "# parâmetros do RM3:\n",
    "# fb_terms é o número de termos de feedback,\n",
    "# fb_docs é o número de documentos de feedback,\n",
    "# original_query_weight é o peso da query original\n",
    "searcher.set_rm3(fb_terms=12, fb_docs=50, original_query_weight=0.6)\n",
    "\n",
    "# inicializando o cross-encoder com o modelo pré-treinado\n",
    "# melhor modelo de cross-encoder\n",
    "ce = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "\n",
    "rows_out = []\n",
    "\n",
    "with TEST_PATH.open() as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in tqdm(reader, desc=\"Queries\"):\n",
    "        qid, query = row[\"QueryId\"], row[\"Query\"]\n",
    "\n",
    "        # inicialmente uso rm3 + bm25 para achar candidatos relevantes para a query\n",
    "        hits = searcher.search(query, CAND_K)\n",
    "        cand_ids  = [h.docid for h in hits]\n",
    "\n",
    "        # agora que tenho os candidatos, uso o cross-encoder para re-ranquear esses candidatos\n",
    "        texts = [searcher.doc(did).raw() for did in cand_ids]\n",
    "        ce_scores = ce.predict([(query, t) for t in texts], batch_size=32)\n",
    "        ce_dict = dict(zip(cand_ids, ce_scores))\n",
    "\n",
    "        # agora só uso os scores do cross-encoder\n",
    "        final_scores = {d: ce_dict[d] for d in cand_ids}\n",
    "    \n",
    "        # pego os melhores candidatos\n",
    "        top_ids = sorted(final_scores, key=final_scores.get, reverse=True)[:FINAL_K]\n",
    "        rows_out.extend([[qid, did] for did in top_ids])\n",
    "\n",
    "# salvo os resultados obtidos \n",
    "with SUBM_PATH4.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"QueryId\", \"EntityId\"])\n",
    "    writer.writerows(rows_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc44c2c",
   "metadata": {},
   "source": [
    "## Submissão 5\n",
    "\n",
    "Rodei com um modelo de CrossEncoder menor, mas que segundo os rankings apresentam os mesmos resultados que o modelo maior.\n",
    "\n",
    "A grande vantagem vem do fato que roda mais rápido, o que permite aumentar o número de documentos candidatos que serão passados para o modelo de CrossEncoder.\n",
    "\n",
    "Também alterei os parâmetros do BM25, de maneira a tentar suavizar a normalização por tamanho e ao mesmo tempo diminuir um pouco o impacto do termo de frequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bfba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBM_PATH5 = Path(\"submission5.csv\")\n",
    "\n",
    "# parametros do pipeline de ranking\n",
    "CAND_K = 4000   # numero de candidatos que recupero inicialmente\n",
    "FINAL_K= 100    # numero final de entidades\n",
    "W_CE = 0.8    # peso do cross-encoder na interpolação com o bm25\n",
    "\n",
    "# agora sim inicio das tarefas de ranqueamento\n",
    "searcher = LuceneSearcher(str(INDEX_DIR))\n",
    "\n",
    "# parametros do bm25, k1 controla a sensibilidade ao tamanho do documento, b controla a normalização\n",
    "searcher.set_bm25(k1=0.8, b=0.2) \n",
    "\n",
    "# parâmetros do RM3:\n",
    "# fb_terms é o número de termos de feedback,\n",
    "# fb_docs é o número de documentos de feedback,\n",
    "# original_query_weight é o peso da query original\n",
    "searcher.set_rm3(fb_terms=10, fb_docs=50, original_query_weight=0.5) \n",
    "\n",
    "# inicializando o cross-encoder com o modelo pré-treinado\n",
    "# melhor modelo de cross-encoder\n",
    "ce = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# como vou fazer interpolação dos modelos, preciso normalizar os scores dos metodos para poder agregalos\n",
    "def normalize_scores(d):\n",
    "    vals = np.array(list(d.values()))\n",
    "    return {k: (v - vals.min()) / (np.ptp(vals) + 1e-9) for k, v in d.items()}\n",
    "\n",
    "rows_out = []\n",
    "\n",
    "with TEST_PATH.open() as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in tqdm(reader, desc=\"Queries\"):\n",
    "        qid, query = row[\"QueryId\"], row[\"Query\"]\n",
    "\n",
    "        # inicialmente uso rm3 + bm25 para achar candidatos relevantes para a query\n",
    "        hits = searcher.search(query, CAND_K)\n",
    "        cand_ids  = [h.docid for h in hits]\n",
    "        bm25_dict = {h.docid: h.score for h in hits}\n",
    "\n",
    "        # agora que tenho os candidatos, uso o cross-encoder para re-ranquear esses candidatos\n",
    "        texts = [searcher.doc(did).raw() for did in cand_ids]\n",
    "        ce_scores = ce.predict([(query, t) for t in texts], batch_size=32)\n",
    "        ce_dict = dict(zip(cand_ids, ce_scores))\n",
    "\n",
    "        # jutno os scores com interpolação\n",
    "        b_norm = normalize_scores(bm25_dict)\n",
    "        c_norm = normalize_scores(ce_dict)\n",
    "        final_scores = {d: W_CE*c_norm[d] + (1-W_CE)*b_norm[d] for d in cand_ids}\n",
    "\n",
    "        # pego os melhores candidatos\n",
    "        top_ids = sorted(final_scores, key=final_scores.get, reverse=True)[:FINAL_K]\n",
    "        rows_out.extend([[qid, did] for did in top_ids])\n",
    "\n",
    "# salvo os resultados obtidos \n",
    "with SUBM_PATH2.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"QueryId\", \"EntityId\"])\n",
    "    writer.writerows(rows_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
